# -*- coding: utf-8 -*-
"""KNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KXZXz0bo0dfmbXZVHQGXv_X4V2TgOa0I
"""

# Cell 1 – Import Libraries & Load Dataset

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Load dataset
df = pd.read_csv('/content/Zoo.csv')  # adjust path if needed
df.head()

# Cell 2 – Basic EDA (Corrected for Zoo Dataset)

print("Dataset Shape:", df.shape)
print("\nDataset Info:")
print(df.info())

print("\nMissing Values per Column:")
print(df.isnull().sum())

print("\nStatistical Summary:")
print(df.describe())

# Class distribution (target column is 'type')
plt.figure(figsize=(6,4))
sns.countplot(x='type', data=df, palette='viridis')
plt.title('Class Distribution (Animal Types)')
plt.xlabel('Animal Type')
plt.ylabel('Count')
plt.show()

# Cell 3 – Data Visualization (Final Fixed)

# Drop non-numeric columns for correlation analysis
numeric_df = df.drop(columns=['animal name'], errors='ignore')

# Correlation heatmap (numeric features only)
plt.figure(figsize=(10,8))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlation Heatmap (Numeric Features Only)")
plt.show()

# Boxplot for numeric feature distributions (excluding target 'type')
plt.figure(figsize=(12,6))
sns.boxplot(data=numeric_df.drop(columns=['type'], errors='ignore'))
plt.title("Boxplots for Feature Distributions")
plt.xticks(rotation=90)
plt.show()

# Pairplot for selected features with target
selected_features = ['hair', 'feathers', 'eggs', 'milk', 'airborne', 'aquatic', 'type']
sns.pairplot(df[selected_features], hue='type', palette='husl')
plt.suptitle("Pairplot of Selected Features vs Type", y=1.02)
plt.show()

# Cell 4 – Data Preprocessing (Missing Values & Outliers)

# Copy dataset to avoid modifying original
df_clean = df.copy()

# Drop the 'animal name' column as it is not useful for modeling
df_clean.drop(columns=['animal name'], inplace=True, errors='ignore')

# Check for missing values
print("Missing Values per Column:\n", df_clean.isnull().sum())

# Handle missing values (if any)
df_clean.fillna(df_clean.median(numeric_only=True), inplace=True)

# Identify numeric columns (excluding target)
numeric_cols = df_clean.drop(columns=['type']).select_dtypes(include=['int64', 'float64']).columns

# Detect outliers using the IQR method
Q1 = df_clean[numeric_cols].quantile(0.25)
Q3 = df_clean[numeric_cols].quantile(0.75)
IQR = Q3 - Q1

# Define a mask for non-outlier data
non_outliers = ~((df_clean[numeric_cols] < (Q1 - 1.5 * IQR)) | (df_clean[numeric_cols] > (Q3 + 1.5 * IQR))).any(axis=1)
df_clean = df_clean[non_outliers]

print("\nDataset shape after outlier removal:", df_clean.shape)

# Feature scaling – Standardization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X = df_clean.drop('type', axis=1)
y = df_clean['type']

X_scaled = scaler.fit_transform(X)

print("\nData preprocessing complete. Feature matrix and target variable ready for model training.")

# Cell 5 – Train-Test Split and KNN Model Training

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

# Split data into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set shape: {X_train.shape}")
print(f"Testing set shape: {X_test.shape}")

# Choose optimal K (we’ll start with k=5)
k = 5
knn = KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=2)  # Euclidean distance

# Train the model
knn.fit(X_train, y_train)

# Predictions
y_pred = knn.predict(X_test)

# Evaluate model performance
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred, average='weighted')
rec = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("\nKNN Model Performance Metrics:")
print(f"Accuracy  : {acc:.4f}")
print(f"Precision : {prec:.4f}")
print(f"Recall    : {rec:.4f}")
print(f"F1-Score  : {f1:.4f}")

# Detailed classification report
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(7,5))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title(f"KNN (k={k}) - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Cell 6 – Decision Boundary Visualization using PCA

from sklearn.decomposition import PCA

# Reduce features to 2D using PCA for visualization
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Retrain KNN on 2D transformed data for visualization
knn_2d = KNeighborsClassifier(n_neighbors=5)
knn_2d.fit(X_train_pca, y_train)

# Create meshgrid for decision boundary
x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1
y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),
                     np.arange(y_min, y_max, 0.05))

# Predict over grid
Z = knn_2d.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot decision boundary
plt.figure(figsize=(10,7))
plt.contourf(xx, yy, Z, alpha=0.3, cmap='Spectral')
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='Spectral', edgecolor='k', label='Training')
plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test, cmap='cool', edgecolor='k', marker='x', label='Testing')
plt.title("KNN Decision Boundaries (After PCA to 2D)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend()
plt.show()

